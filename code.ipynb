{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, ymin, xmin , ymax , xmax, objness=None, classes=None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "\n",
    "        return self.label\n",
    "\n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "\n",
    "        return self.score\n",
    "    \n",
    "    \n",
    "# x1,y1 ----------\n",
    "#       |        |\n",
    "#       |        |\n",
    "#       |        |\n",
    "#       ----------x2,y2\n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2, x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2, x4) - x3\n",
    "\n",
    "# IoU = (A intersection B) / (A union B)\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap(\n",
    "        [box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap(\n",
    "        [box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    if union == 0: \n",
    "        return 0\n",
    "    return float(intersect) / union\n",
    "\n",
    "\n",
    "def create_box(a):\n",
    "    return BoundBox(a[0],a[1],a[2],a[3]) # y1, x1, y2, x2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting Started](./mark_down/image/confusion-matrix.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 7 88\n",
      "16 7 74\n",
      "19 8 61\n",
      "20 3 62\n",
      "12 9 65\n",
      "22 8 60\n",
      "12 4 69\n",
      "11 11 68\n",
      "18 4 62\n",
      "16 5 60\n",
      "17 7 59\n",
      "11 4 64\n",
      "4 7 77\n",
      "10 10 61\n",
      "7 10 67\n",
      "10 13 61\n",
      "7 17 60\n",
      "10 11 63\n",
      "12 10 62\n",
      "10 9 60\n",
      "10 15 56\n",
      "5 13 66\n",
      "12 15 55\n",
      "11 13 60\n",
      "11 7 65\n",
      "10 11 58\n",
      "12 11 60\n",
      "13 12 61\n",
      "11 12 63\n",
      "10 19 51\n",
      "17 10 52\n",
      "6 14 55\n",
      "5 13 56\n",
      "7 11 56\n",
      "6 10 59\n",
      "7 13 54\n",
      "4 15 60\n",
      "10 13 62\n",
      "5 7 69\n",
      "10 8 62\n",
      "3 7 70\n",
      "3 11 70\n",
      "7 14 64\n",
      "11 7 62\n",
      "14 6 57\n",
      "15 7 56\n",
      "11 8 57\n",
      "9 8 63\n",
      "12 4 60\n",
      "5 4 42\n",
      "9 10 21\n",
      "7 16 15\n",
      "5 12 22\n",
      "8 7 34\n",
      "9 8 25\n",
      "4 12 26\n",
      "2 17 23\n",
      "11 11 20\n",
      "10 6 34\n",
      "3 5 9\n",
      "2 10 5\n",
      "0 9 11\n",
      "3 12 8\n",
      "3 12 11\n",
      "4 11 5\n",
      "4 8 9\n",
      "4 12 5\n",
      "3 13 3\n",
      "3 8 6\n",
      "2 9 4\n",
      "4 9 3\n",
      "2 7 9\n",
      "4 8 2\n",
      "6 8 5\n",
      "3 9 1\n",
      "1 11 4\n",
      "3 10 3\n",
      "2 8 4\n",
      "4 10 2\n",
      "5 4 6\n",
      "4 5 7\n",
      "3 8 5\n",
      "3 9 4\n",
      "2 7 8\n",
      "5 4 5\n",
      "8 5 3\n",
      "12 1 10\n",
      "4 10 2\n",
      "6 8 3\n",
      "4 11 -3\n",
      "4 8 1\n",
      "6 7 1\n",
      "5 8 3\n",
      "4 3 2\n",
      "3 4 3\n",
      "2 6 2\n",
      "3 7 1\n",
      "3 6 4\n",
      "5 6 1\n",
      "6 6 1\n",
      "1 9 -1\n",
      "1 9 -2\n",
      "2 6 0\n",
      "2 4 1\n",
      "1 7 3\n",
      "3 9 -1\n",
      "3 10 -2\n",
      "4 6 6\n",
      "5 7 5\n",
      "4 8 6\n",
      "2 11 4\n",
      "5 5 2\n",
      "8 1 13\n",
      "6 2 13\n",
      "5 2 13\n",
      "5 4 7\n",
      "7 3 7\n",
      "7 5 10\n",
      "8 4 6\n",
      "9 5 11\n",
      "11 6 6\n",
      "8 3 8\n",
      "10 5 4\n",
      "10 6 7\n",
      "9 6 6\n",
      "9 6 4\n",
      "10 5 6\n",
      "9 6 4\n",
      "9 1 39\n",
      "7 2 16\n",
      "8 0 27\n",
      "8 1 27\n",
      "11 1 20\n",
      "14 0 15\n",
      "10 2 22\n",
      "10 1 25\n",
      "7 3 19\n",
      "10 1 19\n",
      "12 2 21\n",
      "8 2 21\n",
      "14 0 18\n",
      "10 0 21\n",
      "11 1 17\n",
      "12 1 13\n",
      "9 1 20\n",
      "9 2 17\n",
      "14 3 19\n",
      "9 3 27\n",
      "11 1 26\n",
      "12 0 18\n",
      "13 1 21\n",
      "12 3 12\n",
      "15 0 11\n",
      "12 2 15\n",
      "12 4 10\n",
      "10 1 16\n",
      "12 3 14\n",
      "8 2 33\n",
      "9 2 21\n",
      "12 1 20\n",
      "10 2 15\n",
      "14 1 15\n",
      "12 2 22\n",
      "9 0 17\n",
      "10 2 17\n",
      "8 7 18\n",
      "5 7 16\n",
      "6 4 20\n",
      "8 2 16\n",
      "12 1 9\n",
      "12 2 13\n",
      "11 4 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.290001053005673,\n",
       " 0.3303910615017583,\n",
       " 0.3780632453821706,\n",
       " 0.43242853875378356,\n",
       " 0.27792639254433277)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row different together\n",
    "def matrix_confusion(matrix):\n",
    "    convert =[]\n",
    "    amount_row = len(matrix)\n",
    "    amount_col = len(matrix[0])\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            convert.append([i,j,matrix[i][j]])\n",
    "    df = pd.DataFrame(convert, columns=['row','col','score'])\n",
    "    # print(df)\n",
    "    # convert row, col, score to matrix\n",
    "    convert = df.values\n",
    "    convert = convert.tolist()\n",
    "    convert = sorted(convert, key=lambda x: x[2], reverse=True)\n",
    "    # print(convert)\n",
    "    \n",
    "    dict_col = []\n",
    "    dict_row = []\n",
    "    values = []\n",
    "    for i in range(len(convert)):\n",
    "        if str(convert[i][0]) not in dict_row and str(convert[i][1]) not in dict_col:\n",
    "            dict_col.append(str(convert[i][1]))\n",
    "            dict_row.append(str(convert[i][0]))\n",
    "            values.append(float(convert[i][2]))\n",
    "    \n",
    "    values = pd.DataFrame({'row':dict_row, 'col':dict_col, 'score':values})\n",
    "    # if IoU ≥0.5, classify the object detection as True Positive(TP)\n",
    "    acc_values = values.loc[values['score'] >= 0.5]\n",
    "    TP = len(acc_values)\n",
    "    # if Iou <0.5, then it is a wrong detection and classify it as False Positive(FP)\n",
    "    FP = len(values) - len(acc_values)\n",
    "    if amount_col > amount_row:\n",
    "        FP = FP + (amount_col - amount_row)\n",
    "    # When a ground truth is present in the image and model failed to detect the object, classify it as False Negative(FN).\n",
    "    FN = amount_row - amount_col\n",
    "    # True Negative (TN): TN is every part of the image where we did not predict an object. This metrics is not useful for object detection, hence we ignore TN.\n",
    "    return TP, FP, FN\n",
    "\n",
    "def precision_recall(TP, FP, FN):\n",
    "    # precision = TP / (TP + FP)\n",
    "    # recall = TP / (TP + FN)\n",
    "    if TP + FN == 0 and TP + FP ==0:\n",
    "        return 0,0\n",
    "    if TP + FP == 0:\n",
    "        return 0,TP / (TP + FN)\n",
    "    \n",
    "    if TP + FN == 0:\n",
    "        return TP / (TP + FP), 0\n",
    "    return TP / (TP + FP), TP / (TP + FN)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def accuracy(TP, FP, FN):\n",
    "    if TP + FP + FN == 0:\n",
    "        return 0\n",
    "    return TP / (TP + FP + FN)\n",
    "\n",
    "def scrore_frame_x(dict_trust,dict_predict):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    acc_ls = []\n",
    "    f1_score_ls = []\n",
    "    precision_ls = []\n",
    "    recall_ls = []\n",
    "    for key in dict_trust:\n",
    "        if key in dict_predict:\n",
    "            matrix_score = []\n",
    "            \n",
    "            for i in range(len(dict_trust[key])):\n",
    "                ls = []\n",
    "                for j in range(len(dict_predict[key])):\n",
    "                    ls.append(bbox_iou(create_box(dict_trust[key][i]),create_box(dict_predict[key][j])))\n",
    "                    \n",
    "                matrix_score.append(ls)\n",
    "                \n",
    "            # matrix_score = np.array(matrix_score)\n",
    "            # matrix_score = pd.DataFrame(matrix_score)\n",
    "            # matrix_score.to_csv('./matrix_score2.csv')\n",
    "            TP, FP, FN = matrix_confusion(matrix_score)\n",
    "            acc_ls.append(accuracy(TP, FP, FN))\n",
    "            precision_ls.append(precision_recall(TP, FP, FN)[0])\n",
    "            recall_ls.append(precision_recall(TP, FP, FN)[1])\n",
    "            f1_score_ls.append(f1_score(precision_recall(TP, FP, FN)[0],precision_recall(TP, FP, FN)[1]))\n",
    "            print(TP,FP,FN)\n",
    "    return np.mean(acc_ls), np.mean(precision_ls), np.mean(recall_ls), np.mean(f1_score_ls)       \n",
    "    \n",
    "\n",
    "def evaluate_count_person(file_trust,file_predict):\n",
    "    df_trust = pd.read_csv(file_trust)\n",
    "    df_predict = pd.read_csv(file_predict)\n",
    "    scores = []\n",
    "    for frame in range(len(df_trust['frame_x'])):\n",
    "    # print(df_trust['bounding_box'][0])\n",
    "    # print(df_predict['bounding_box'][0])        \n",
    "        import ast\n",
    "        dict_trust = ast.literal_eval(df_trust['bounding_box'][frame])\n",
    "        dict_predict = ast.literal_eval(df_predict['bounding_box'][frame])\n",
    "        scores.append(scrore_frame_x(dict_trust,dict_predict))\n",
    "        \n",
    "    return np.mean(scores[0]), np.mean(scores[1]), np.mean(scores[2]), np.mean(scores[3])\n",
    "\n",
    "evaluate_count_person('./trust_csv/1.csv','./predict_csv/1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row different together\n",
    "def matrix_(matrix):\n",
    "    convert =[]\n",
    "    amount_row = len(matrix)\n",
    "    amount_col = len(matrix[0])\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            convert.append([i,j,matrix[i][j]])\n",
    "    df = pd.DataFrame(convert, columns=['row','col','score'])\n",
    "    # print(df)\n",
    "    # convert row, col, score to matrix\n",
    "    convert = df.values\n",
    "    convert = convert.tolist()\n",
    "    convert = sorted(convert, key=lambda x: x[2], reverse=True)\n",
    "    # print(convert)\n",
    "    \n",
    "    dict_col = []\n",
    "    dict_row = []\n",
    "    values = []\n",
    "    for i in range(len(convert)):\n",
    "        if str(convert[i][0]) not in dict_row and str(convert[i][1]) not in dict_col:\n",
    "            dict_col.append(str(convert[i][1]))\n",
    "            dict_row.append(str(convert[i][0]))\n",
    "            values.append(float(convert[i][2]))\n",
    "    \n",
    "    values = pd.DataFrame({'row':dict_row, 'col':dict_col, 'score':values})\n",
    "    # if IoU ≥0.5, classify the object detection as True Positive(TP)\n",
    "    acc_values = values.loc[values['score'] >= 0.5]\n",
    "    TP = len(acc_values)\n",
    "    # if Iou <0.5, then it is a wrong detection and classify it as False Positive(FP)\n",
    "    FP = len(values) - len(acc_values)\n",
    "    if amount_col > amount_row:\n",
    "        FP = FP + (amount_col - amount_row)\n",
    "    # When a ground truth is present in the image and model failed to detect the object, classify it as False Negative(FN).\n",
    "    FN = amount_row - amount_col\n",
    "    # True Negative (TN): TN is every part of the image where we did not predict an object. This metrics is not useful for object detection, hence we ignore TN.\n",
    "    return TP, FP, FN\n",
    "\n",
    "def confusion_matrix_frame_x(dict_trust,dict_predict):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    matrix_score = []\n",
    "    for key_t in dict_trust:\n",
    "        # if key in dict_predict:\n",
    "        \n",
    "        \n",
    "        for i in range(len(dict_trust[key_t])):\n",
    "            ls = []\n",
    "            for key_p in dict_predict:\n",
    "                for j in range(len(dict_predict[key_p])):\n",
    "                    ls.append([key_t,key_p,bbox_iou(create_box(dict_trust[key_t][i]),create_box(dict_predict[key_p][j]))])\n",
    "\n",
    "            matrix_score.append(ls)\n",
    "        # matrix_score = np.array(matrix_score)\n",
    "        # print(matrix_score)\n",
    "        # matrix_score = pd.DataFrame(matrix_score)\n",
    "        # matrix_score.to_csv('./matrix_score1.csv')\n",
    "        TP, FP, FN = matrix_(matrix_score)\n",
    "        print(TP, FP, FN)\n",
    "            \n",
    "    \n",
    "\n",
    "def evaluate_count_person(file_trust,file_predict):\n",
    "    df_trust = pd.read_csv(file_trust)\n",
    "    df_predict = pd.read_csv(file_predict)\n",
    "    # for frame in range(len(df_trust['frame_x'])):\n",
    "    # print(df_trust['bounding_box'][0])\n",
    "    # print(df_predict['bounding_box'][0])        \n",
    "    import ast\n",
    "    dict_trust = ast.literal_eval(df_trust['bounding_box'][0])\n",
    "    dict_predict = ast.literal_eval(df_predict['bounding_box'][0])\n",
    "    confusion_matrix_frame_x(dict_trust,dict_predict)\n",
    "    \n",
    "evaluate_count_person('./1.csv','./1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a3ffae8020a817511732d4d0fe02060948831585d778b076daf6741c73f247a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
